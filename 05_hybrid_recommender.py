import pandas as pd
import numpy as np
from scipy.sparse import csr_matrix
from sklearn.metrics.pairwise import cosine_similarity, linear_kernel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD  # Importation pour l'optimisation TF-IDF (LSI)
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import os
import sys

# --- HYPERPARAMÃˆTRES GLOBALES ---
SUBJECT_WEIGHT_FACTOR = 3  # Poids des sujets pour SBERT (doit Ãªtre >= 1)
TFIDF_SUBJECT_WEIGHT = 5  # Poids des sujets pour TF-IDF
TFIDF_SVD_COMPONENTS = 500  # Dimensions latentes pour le TF-IDF (LSI)
EMBEDDINGS_FILE = 'sbert_embeddings_ultimate.npy'

# ==============================================================================
# SECTION 0: CHARGEMENT ET SPLIT (VÃ©rifiÃ© et CorrigÃ©)
# ==============================================================================

print("\n--- 0. CHARGEMENT ET SPLIT ---")

# 1. Chargement des donnÃ©es brutes
try:
    # Utilisation des chemins relatifs corrigÃ©s
    interactions_df = pd.read_csv('kaggle_data/interactions_train.csv')
    items_df = pd.read_csv('kaggle_data/items.csv')
except FileNotFoundError:
    print("ERREUR FATALE: Le chemin 'kaggle_data/' est incorrect. VÃ©rifiez le chemin ou la prÃ©sence des fichiers.")
    sys.exit()

# 2. Renommage et Nettoyage
if 'user_id' in interactions_df.columns:
    interactions_df = interactions_df.rename(columns={'user_id': 'u', 'item_id': 'i', 'timestamp': 't'})

# 3. Le Split Temporel (MÃ©thode 80/20 par timestamp)
interactions_df = interactions_df.sort_values(["u", "t"])
interactions_df["pct_rank"] = interactions_df.groupby("u")["t"].rank(pct=True, method='dense')

train_data = interactions_df[interactions_df["pct_rank"] < 0.8]
test_data = interactions_df[interactions_df["pct_rank"] >= 0.8]

print("âœ… DonnÃ©es chargÃ©es et splittÃ©es correctement.")

# ==============================================================================
# SECTION 1: MATRICE FONDATION (R_train)
# ==============================================================================

print("\n--- 1. CRÃ‰ATION DE R_train (User x Item) ---")

# MAPPING (BasÃ© sur l'univers complet des IDs pour la cohÃ©rence)
unique_users = interactions_df['u'].unique()
unique_items = interactions_df['i'].unique()
user_to_idx = {u: idx for idx, u in enumerate(unique_users)}
item_to_idx = {i: idx for idx, i in enumerate(unique_items)}
idx_to_item = {idx: i for i, idx in item_to_idx.items()}

train_rows = train_data['u'].map(user_to_idx).values
train_cols = train_data['i'].map(item_to_idx).values
interactions = np.ones(len(train_data))

R_train = csr_matrix((interactions, (train_rows, train_cols)),
                     shape=(len(unique_users), len(unique_items)))

print(f"âœ… R_train (CF Base) construite. Shape: {R_train.shape}")

# ==============================================================================
# SECTION 2: MODÃˆLE A - ITEM-ITEM CF (Comportement)
# ==============================================================================

print("\n--- 2. MODÃˆLE A: SimilaritÃ© CF (Interactions) ---")
item_sim_matrix_cf = cosine_similarity(R_train.T, dense_output=True)
np.fill_diagonal(item_sim_matrix_cf, 0)
print(f"âœ… Matrice A (CF) prÃªte. Shape: {item_sim_matrix_cf.shape}")

# ==============================================================================
# SECTION 3: PRÃ‰PARATION DU CONTENU (Optimisations)
# ==============================================================================

print("\n--- 3. PRÃ‰PARATION DU CONTENU (Soupes de mots optimisÃ©es) ---")

# 1. Nettoyage des colonnes SÃ›RES
items_df['Title'] = items_df['Title'].fillna('')
items_df['Author'] = items_df['Author'].fillna('')
items_df['Subjects'] = items_df['Subjects'].fillna('')


# NOTE: Si 'summary' existe, la charger ici: items_df['summary'] = items_df['summary'].fillna('')

# 2. CrÃ©ation de la SOUPE TF-IDF (Optimisation : Poids et SÃ©parateur)
def create_tfidf_soup(x):
    # RÃ©pÃ©ter les sujets 5 fois (le poids maximal)
    return (x['Title'] + ' ') + (x['Author'] + ' ') + (x['Subjects'] + ' ') * TFIDF_SUBJECT_WEIGHT


items_df['tfidf_soup'] = items_df.apply(create_tfidf_soup, axis=1)


# 3. CrÃ©ation de la SOUPE SBERT (Optimisation : Format lisible par BERT)
def create_sbert_soup(x):
    # RÃ©pÃ©ter les sujets 3 fois (pondÃ©ration)
    subjects_text = (x['Subjects'] + ' ') * SUBJECT_WEIGHT_FACTOR
    # Utilisation d'un format clair pour le modÃ¨le sÃ©mantique
    # NOTE: Adapter si vous ajoutez 'summary' (ex: f"...[SUMMARY] {x['summary']}")
    return f"{x['Title']} [SEP] {x['Author']} [SEP] {subjects_text}"


items_df['sbert_soup'] = items_df.apply(create_sbert_soup, axis=1)

print("âœ… Soupes TF-IDF et SBERT crÃ©Ã©es.")

# ==============================================================================
# SECTION 4: MODÃˆLES B & C - MATRICES DE SIMILARITÃ‰
# ==============================================================================

# --- MODÃˆLE B: TF-IDF + SVD (Latent Semantic Indexing - NOUVEAU CHAMPION) ---
print("\n--- 4A. MODÃˆLE B: Matrice TF-IDF + SVD (OptimisÃ©) ---")


# On utilise un custom tokenizer pour les sujets (sÃ©parÃ©s par ;) et on ignore les mots trop rares (min_df=5)
def custom_tokenizer(text):
    # SÃ©pare par espace ou point-virgule, puis enlÃ¨ve le vide
    return [t.strip() for t in text.replace(';', ' ').split() if t.strip()]


tfidf = TfidfVectorizer(tokenizer=custom_tokenizer, min_df=5, ngram_range=(1, 2))
tfidf_matrix = tfidf.fit_transform(items_df['tfidf_soup'])

# Application de la RÃ©duction de Dimension (LSI)
svd_transformer = TruncatedSVD(n_components=TFIDF_SVD_COMPONENTS, random_state=42)
tfidf_svd_matrix = svd_transformer.fit_transform(tfidf_matrix)

# Calcul de la SimilaritÃ© sur les vecteurs SVD (plus court, moins bruitÃ©)
item_sim_matrix_tfidf_svd = cosine_similarity(tfidf_svd_matrix, tfidf_svd_matrix)
np.fill_diagonal(item_sim_matrix_tfidf_svd, 0)
print(f"âœ… Matrice B (TF-IDF+SVD) prÃªte. Shape: {item_sim_matrix_tfidf_svd.shape}")

# --- MODÃˆLE C: SBERT (SÃ©mantique) ---
print("\n--- 4B. MODÃˆLE C: Matrice SBERT (NÃ©cessite GPU/Caching) ---")

if os.path.exists(EMBEDDINGS_FILE):
    embeddings = np.load(EMBEDDINGS_FILE)
    print(f"âœ… Embeddings SBERT chargÃ©s ({embeddings.shape}).")
else:
    print("âš ï¸ Encoudage SBERT lancÃ© (vÃ©rifiez l'activation du GPU).")
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    embeddings = model.encode(items_df['sbert_soup'].tolist(), show_progress_bar=True, convert_to_tensor=False)
    np.save(EMBEDDINGS_FILE, embeddings)
    print("âœ… Embeddings sauvegardÃ©s.")

item_sim_matrix_sbert = cosine_similarity(embeddings)
np.fill_diagonal(item_sim_matrix_sbert, 0)
print(f"âœ… Matrice C (SBERT) prÃªte. Shape: {item_sim_matrix_sbert.shape}")

# ==============================================================================
# SECTION 5: ALIGNEMENT DES MATRICES B & C SUR LA MATRICE A
# ==============================================================================

print("\n--- 5. ALIGNEMENT DES MATRICES ---")
# 1. CrÃ©er le plan d'alignement
content_id_to_row_idx = pd.Series(items_df.index, index=items_df['i']).to_dict()
n_items_cf = len(idx_to_item)
aligned_content_indices = []

for cf_idx in range(n_items_cf):
    item_id = idx_to_item[cf_idx]
    # Robustesse : VÃ©rification de l'existence de l'ID dans le contenu
    if item_id in content_id_to_row_idx:
        content_idx = content_id_to_row_idx[item_id]
        aligned_content_indices.append(content_idx)
    else:
        # Si un livre est dans le Train CF mais pas dans items_df, c'est un problÃ¨me.
        # On suppose qu'ils sont tous lÃ  pour l'instant.
        pass

    # 2. Appliquer l'alignement sur B (TF-IDF+SVD) et C (SBERT)
# NOTE: Utilisation de la nouvelle matrice optimisÃ©e!
content_sim_aligned_tfidf_svd = item_sim_matrix_tfidf_svd[np.ix_(aligned_content_indices, aligned_content_indices)]
content_sim_aligned_sbert = item_sim_matrix_sbert[np.ix_(aligned_content_indices, aligned_content_indices)]

print("âœ… Matrices B (TFIDF+SVD) et C (SBERT) alignÃ©es et prÃªtes pour la fusion.")

# ==============================================================================
# SECTION 6 & 7: Ã‰VALUATION ET TUNING FINAL (Logique inchangÃ©e, RAM-SAFE)
# ==============================================================================

# On garde les fonctions RAM-SAFE.
# (Code des fonctions get_user_recommendations_safe et evaluate_map_at_10_3_MODELS)

item_popularity = np.array(R_train.sum(axis=0)).flatten()
popular_indices = item_popularity.argsort()[-10:][::-1]


def get_user_recommendations_safe(user_idx, interaction_matrix, sim_matrix, top_k=10):
    user_history = interaction_matrix.getrow(user_idx)
    recommendations = []

    if user_history.nnz > 0:
        scores = user_history.dot(sim_matrix)

        if hasattr(scores, 'toarray'):
            scores = scores.toarray().flatten()
        else:
            scores = scores.flatten()

        seen_indices = user_history.indices
        scores[seen_indices] = -np.inf

        top_indices_hybride = scores.argsort()[-top_k:][::-1]

        for idx in top_indices_hybride:
            if scores[idx] > 0: recommendations.append(idx)

    for pop_idx in popular_indices:
        if len(recommendations) >= top_k: break
        if (pop_idx not in recommendations) and (pop_idx not in user_history.indices):
            recommendations.append(pop_idx)

    return recommendations[:top_k]


def evaluate_map_at_10_3_MODELS(test_df, R_train,
                                matrix_A_cf, matrix_B_tfidf_svd, matrix_C_sbert,
                                user_to_idx, idx_to_item,
                                weights, top_k=10):
    alpha, beta, gamma = weights

    test_ground_truth = test_df.groupby('u')['i'].apply(set).to_dict()
    average_precisions = []
    common_users = [u for u in test_ground_truth.keys() if u in user_to_idx]

    for user_id in tqdm(common_users, desc=f"Eval w={weights}", leave=False):
        u_idx = user_to_idx[user_id]
        user_history = R_train.getrow(u_idx)
        recommendations = []

        if user_history.nnz > 0:
            # Calcul des scores individuels
            scores_cf = user_history.dot(matrix_A_cf).flatten()
            scores_tfidf = user_history.dot(matrix_B_tfidf_svd).flatten()  # <--- Nouvelle matrice optimisÃ©e
            scores_sbert = user_history.dot(matrix_C_sbert).flatten()

            # Fusion
            scores_hybrid = (alpha * scores_cf) + (beta * scores_tfidf) + (gamma * scores_sbert)

            # Exclusion et Top-K
            seen_indices = user_history.indices
            scores_hybrid[seen_indices] = -np.inf
            top_indices_hybride = scores_hybrid.argsort()[-top_k:][::-1]
            for idx in top_indices_hybride:
                if scores_hybrid[idx] > 0: recommendations.append(idx)

        # Roue de secours (PopularitÃ©)
        for pop_idx in popular_indices:
            if len(recommendations) >= top_k: break
            if (pop_idx not in recommendations) and (pop_idx not in user_history.indices):
                recommendations.append(pop_idx)

        rec_items = [idx_to_item[i] for i in recommendations[:top_k]]
        true_items = test_ground_truth[user_id]
        hits = 0
        sum_precisions = 0
        for rank, item in enumerate(rec_items, start=1):
            if item in true_items:
                hits += 1
                sum_precisions += hits / rank

        if not true_items:
            ap = 0
        else:
            ap = sum_precisions / min(len(true_items), 10)
        average_precisions.append(ap)

    return np.mean(average_precisions)


# --- SECTION 7: TUNING FINAL ---

combinations_to_test = [
    [0.1, 0.8, 0.1],  # PrioritÃ© TFIDF+SVD (Ancien champion)
]

best_score = 0
best_weights = []

print("\n\n--- ðŸš€ LANCEMENT DU GRAND TEST DE FUSION (A+B+C) ---")

for weights in combinations_to_test:
    score_hybrid = evaluate_map_at_10_3_MODELS(
        test_data, R_train,
        item_sim_matrix_cf, content_sim_aligned_tfidf_svd, content_sim_aligned_sbert,
        # <--- Utilisation des matrices optimisÃ©es
        user_to_idx, idx_to_item,
        weights
    )

    print(f"ðŸ† Score obtenu (w={weights}): {score_hybrid:.5f}")

    if score_hybrid > best_score:
        best_score = score_hybrid
        best_weights = weights

print("\n" + "=" * 70)
print(f"ðŸ”¥ðŸ”¥ðŸ”¥ MEILLEUR SCORE HYBRIDE : {best_score:.5f}")
print(f"       (Obtenu avec [CF, TF-IDF+SVD, SBERT] = {best_weights})")
print(f"       Baseline Ã  battre : 0.1452")
print("=" * 70)